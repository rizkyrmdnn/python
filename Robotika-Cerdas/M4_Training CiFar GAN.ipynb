{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Rizky Ramdhani Koswara\n","### 11122300\n","### 4KA25"],"metadata":{"id":"F_MhJcjI2Wt7"}},{"cell_type":"markdown","metadata":{"id":"NJp-D51g0IDd"},"source":["## **1) Importing Python Packages for GAN**\n"]},{"cell_type":"code","metadata":{"id":"1k5mFBuzzl2a"},"source":["from keras.datasets import cifar10, mnist\n","from keras.models import Sequential\n","from keras.layers import Reshape\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import Dropout\n","\n","# Import LeakyReLU directly from keras.layers\n","from keras.layers import LeakyReLU\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","!mkdir generated_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 1: Impor Library**\n","\n","**Logika Kode:**\n","Blok ini berfungsi untuk mengimpor semua \"alat\" atau *library* Python yang kita butuhkan untuk membangun dan melatih GAN:\n","* `keras.datasets`: Untuk memuat dataset **CIFAR-10** (gambar berwarna kecil) dan MNIST (meskipun MNIST tidak digunakan di sini).\n","* `keras.models`, `keras.layers`: Komponen utama dari Keras untuk membangun arsitektur jaringan saraf (Sequential, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, LeakyReLU, Dropout). `Conv2D` dan `Conv2DTranspose` penting karena kita bekerja dengan gambar.\n","* `tensorflow.keras.optimizers`: Mengimpor optimizer `Adam`, yang akan mengatur bagaimana model belajar selama pelatihan.\n","* `numpy`: Library standar untuk operasi numerik, terutama untuk mengelola data gambar dalam bentuk array.\n","* `!mkdir generated_images`: Ini adalah perintah *shell* (bukan Python murni) untuk membuat direktori (folder) baru bernama `generated_images`. Folder ini akan digunakan untuk menyimpan gambar-gambar yang dihasilkan oleh Generator selama pelatihan.\n","\n","**Output/Efek:**\n","* Semua *library* yang diperlukan dimuat ke dalam memori, siap digunakan.\n","* Sebuah folder bernama `generated_images` dibuat di direktori kerja saat ini. Jika folder tersebut sudah ada, perintah `mkdir` mungkin akan menampilkan pesan error (biasanya tidak berbahaya dan bisa diabaikan)."],"metadata":{"id":"mtw2FYLK3bRt"}},{"cell_type":"markdown","metadata":{"id":"Yr-eZOzg0X79"},"source":["## **2) Parameters for Neural Networks & Data**"]},{"cell_type":"code","metadata":{"id":"RThZMDruz9cB"},"source":["img_width = 32\n","img_height = 32\n","channels = 3\n","img_shape = (img_width, img_height, channels)\n","latent_dim = 100\n","adam = Adam(learning_rate=0.0002)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 2: Parameter**\n","\n","**Logika Kode:**\n","Blok ini mendefinisikan variabel-variabel penting (parameter) yang akan mengontrol struktur model dan proses pelatihan:\n","* `img_width`, `img_height`, `channels`: Menentukan dimensi gambar target. Untuk dataset CIFAR-10, ukurannya adalah **32x32 piksel** dengan **3 channel warna** (Merah, Hijau, Biru - RGB).\n","* `img_shape`: Menggabungkan dimensi di atas menjadi satu tuple `(32, 32, 3)` yang akan digunakan untuk mendefinisikan bentuk input/output layer gambar.\n","* `latent_dim`: Ukuran **ruang laten** (input acak untuk Generator), diatur ke **100**. Ini berarti Generator akan membuat gambar berdasarkan 100 angka acak sebagai \"inspirasi\".\n","* `adam`: Menginisialisasi optimizer **Adam** dengan *learning rate* (laju belajar) tertentu (`0.0002`). *Learning rate* mengontrol seberapa besar penyesuaian yang dilakukan pada model di setiap langkah pelatihan.\n","\n","**Output/Efek:**\n","* Variabel-variabel ini disimpan dalam memori. Nilai-nilai ini akan digunakan saat membangun arsitektur Generator dan Diskriminator, serta saat mengompilasi model. Tidak ada output visual di sini, tetapi variabel siap pakai."],"metadata":{"id":"vrqLxx7w3gWD"}},{"cell_type":"markdown","metadata":{"id":"U3bcJZZg0cqy"},"source":["## **3) Building Generator**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NdiqZpri0iQh","colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"status":"ok","timestamp":1729668988906,"user_tz":-420,"elapsed":423,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}},"outputId":"90569987-1c2b-456a-f12d-3a535506384b"},"source":["def build_generator():\n","    model = Sequential()\n","\n","    # Create first layer, to receive the input\n","    model.add(Dense(256 * 4 * 4, input_dim = latent_dim))\n","    # 256 * 8 * 8; for upscaling the layers,\n","    # initial shape to construct into final shape\n","\n","    # Create default activation function\n","    model.add(LeakyReLU(alpha = 0.2))\n","\n","    # Create reshape layer\n","    model.add(Reshape((4, 4,256)))\n","    # 8,8,256 ; reffers to first layer\n","\n","    # Adding more layers for neurons and better result\n","    model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))\n","    model.add(LeakyReLU(alpha= 0.2))\n","    model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))\n","    model.add(LeakyReLU(alpha= 0.2))\n","    model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))\n","    model.add(LeakyReLU(alpha= 0.2))\n","    # (4,4) >> filter size\n","    # strides = (2,2) >> Convolutional layers, that how NN understand images\n","\n","    # Create Final output layer and forming image shape\n","    # the shape (3, (3,3)) reffers to image shape :\n","    #    >>>  img_shape = (img_width, img_height, channels)\n","    model.add(Conv2D(3, (3,3), activation= 'tanh', padding = 'same'))\n","\n","    #\n","    model.summary()\n","    return model\n","\n","generator = build_generator()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │         \u001b[38;5;34m413,696\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape (\u001b[38;5;33mReshape\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m524,416\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_1 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m262,272\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_2 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m262,272\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │           \u001b[38;5;34m3,459\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">413,696</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,416</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,459</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,466,115\u001b[0m (5.59 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,466,115</span> (5.59 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,466,115\u001b[0m (5.59 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,466,115</span> (5.59 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 3: Membangun Generator**\n","\n","**Logika Kode:**\n","Blok ini mendefinisikan arsitektur **Generator** menggunakan model `Sequential` Keras. Tugas Generator adalah mengubah input acak (`latent_dim`=100) menjadi gambar berukuran 32x32x3 yang mirip gambar CIFAR-10. Proses ini disebut *upsampling*.\n","1.  **Input & Proyeksi Awal**: Layer `Dense` pertama menerima input 100 dimensi dan memproyeksikannya ke ruang yang jauh lebih besar (`256 * 4 * 4 = 4096` neuron). Ini adalah langkah awal untuk menyiapkan data sebelum diubah menjadi format gambar.\n","2.  **Aktivasi & Reshape**: Fungsi aktivasi `LeakyReLU` digunakan (membantu aliran gradien saat training). Kemudian, `Reshape` mengubah output Dense 1D menjadi *feature map* kecil 3D (`4x4x256`). Ini adalah \"kanvas\" awal Generator.\n","3.  **Upsampling (Conv2DTranspose)**: Beberapa layer `Conv2DTranspose` digunakan secara berurutan. Layer ini melakukan kebalikan dari konvolusi biasa; ia memperbesar dimensi spasial (*feature map*) sambil mengurangi/menyesuaikan jumlah channel/filter.\n","    * `4x4x256` -> `8x8x128`\n","    * `8x8x128` -> `16x16x128`\n","    * `16x16x128` -> `32x32x128`\n","    * `LeakyReLU` digunakan setelah setiap *upsampling*. `strides=(2,2)` menggandakan tinggi dan lebar, `padding='same'` memastikan ukurannya pas.\n","4.  **Output Layer (Conv2D)**: Layer `Conv2D` terakhir menghasilkan output akhir dengan 3 channel (sesuai `img_shape`) menggunakan filter 3x3. Fungsi aktivasi **`tanh`** digunakan, menghasilkan nilai piksel dalam rentang **[-1, 1]**.\n","5.  **Ringkasan**: `model.summary()` mencetak detail arsitektur ke layar.\n","\n","**Output/Efek:**\n","* Sebuah model Keras (`Sequential`) yang merepresentasikan Generator dibuat dan disimpan dalam variabel `generator`.\n","* Output sel menunjukkan `model.summary()`: daftar layer, bentuk output setiap layer, dan jumlah parameter (bobot) yang dapat dilatih. Ini memberikan gambaran detail arsitektur yang baru saja didefinisikan. Peringatan (*warnings*) mungkin muncul terkait argumen `input_dim` atau `alpha` yang usang, tapi model tetap dibuat."],"metadata":{"id":"ihBzZ_WP3wCs"}},{"cell_type":"markdown","metadata":{"id":"Bt6QsJCW0mcI"},"source":["## **4) Building Discriminator**"]},{"cell_type":"code","metadata":{"id":"V2JzEAPv0lKt","colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"status":"ok","timestamp":1729668989369,"user_tz":-420,"elapsed":476,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}},"outputId":"50c6f1e9-dde6-455b-d1a5-11f9216601a4"},"source":["def build_discriminator():\n","    model = Sequential()\n","\n","    # Create input layer and filter and stride layer. That makes NN understand image\n","    model.add(Conv2D(64, (3,3), padding = 'same', input_shape = img_shape))\n","\n","    # Adding activation function\n","    model.add(LeakyReLU(alpha = 0.2))\n","    model.add(Conv2D(128, (3,3), padding = 'same'))\n","    model.add(LeakyReLU(alpha = 0.2))\n","    model.add(Conv2D(128, (3,3), padding = 'same'))\n","    model.add(LeakyReLU(alpha = 0.2))\n","    model.add(Conv2D(256, (3,3), padding = 'same'))\n","    model.add(LeakyReLU(alpha = 0.2))\n","    model.add(Flatten())\n","\n","    model.add(Dropout(0.4))\n","\n","    # Create output layer\n","    model.add(Dense(1, activation = 'sigmoid'))\n","\n","    model.summary()\n","    return model\n","\n","discriminator = build_discriminator()\n","discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m1,792\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │         \u001b[38;5;34m262,145\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,145</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m780,545\u001b[0m (2.98 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">780,545</span> (2.98 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m780,545\u001b[0m (2.98 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">780,545</span> (2.98 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 4: Membangun Diskriminator**\n","\n","**Logika Kode:**\n","Blok ini mendefinisikan arsitektur **Diskriminator**, juga menggunakan model `Sequential`. Tugas Diskriminator adalah menerima gambar (asli atau palsu) dan mengklasifikasikannya sebagai \"asli\" (output dekat 1) atau \"palsu\" (output dekat 0). Ini pada dasarnya adalah **CNN (Convolutional Neural Network)** untuk klasifikasi biner.\n","1.  **Input & Feature Extraction (Conv2D)**: Beberapa layer `Conv2D` digunakan untuk memproses gambar input `(32, 32, 3)`. Layer ini berfungsi mengekstrak fitur-fitur visual dari gambar (tepi, tekstur, bentuk).\n","    * Jumlah filter meningkat (64 -> 128 -> 128 -> 256) untuk menangkap fitur yang semakin kompleks.\n","    * `padding='same'` menjaga ukuran spasial gambar tetap 32x32 di setiap layer konvolusi.\n","    * `LeakyReLU` digunakan sebagai fungsi aktivasi setelah setiap `Conv2D`.\n","2.  **Flatten**: Setelah ekstraksi fitur, layer `Flatten` mengubah *feature map* 3D terakhir menjadi vektor 1D yang panjang.\n","3.  **Dropout**: Layer `Dropout(0.4)` secara acak menonaktifkan 40% neuron selama pelatihan. Ini adalah teknik **regularisasi** untuk mencegah *overfitting* (model terlalu hafal data training dan tidak bisa generalisasi ke data baru).\n","4.  **Output Layer**: Layer `Dense` terakhir hanya memiliki **1 neuron** dengan fungsi aktivasi **`sigmoid`**. Sigmoid menghasilkan output antara 0 dan 1, yang ideal untuk merepresentasikan probabilitas keaslian gambar.\n","5.  **Ringkasan & Kompilasi**: `model.summary()` mencetak arsitektur. Kemudian, `discriminator.compile(...)` menyiapkan Diskriminator untuk pelatihan mandiri.\n","    * `loss='binary_crossentropy'`: Fungsi kerugian yang cocok untuk klasifikasi biner (asli vs palsu).\n","    * `optimizer=adam`: Menggunakan optimizer Adam yang sudah didefinisikan.\n","    * `metrics=['accuracy']`: Meminta model untuk melaporkan akurasi selama pelatihan.\n","\n","**Output/Efek:**\n","* Model Keras untuk Diskriminator dibuat dan disimpan dalam variabel `discriminator`.\n","* Model ini dikompilasi, artinya siap untuk dilatih menggunakan metode `train_on_batch`.\n","* Output sel menunjukkan `model.summary()` yang merinci arsitektur Diskriminator. Peringatan tentang `input_shape` mungkin muncul."],"metadata":{"id":"9aAeOdXx30Ug"}},{"cell_type":"markdown","metadata":{"id":"TbcKcKmA0q2S"},"source":["## **5) Connecting Neural Networks to build GAN**"]},{"cell_type":"code","metadata":{"id":"q0Ue3TEd0xLy"},"source":["GAN = Sequential()\n","discriminator.trainable = False\n","GAN.add(generator)\n","GAN.add(discriminator)\n","\n","GAN.compile(loss='binary_crossentropy', optimizer=adam)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPqU8dZDaQmE","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"a8575c39-e36a-4349-ba8a-9fdb6884a4e0","executionInfo":{"status":"ok","timestamp":1729668989370,"user_tz":-420,"elapsed":16,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}}},"source":["GAN.summary()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_2\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │       \u001b[38;5;34m1,466,115\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │         \u001b[38;5;34m780,545\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,466,115</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">780,545</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,246,660\u001b[0m (8.57 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,246,660</span> (8.57 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,466,115\u001b[0m (5.59 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,466,115</span> (5.59 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m780,545\u001b[0m (2.98 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">780,545</span> (2.98 MB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 5: Menghubungkan Jaringan (Membangun GAN)**\n","\n","**Logika Kode:**\n","Blok ini menggabungkan Generator dan Diskriminator menjadi satu model besar, yaitu model **GAN**, yang tujuannya adalah untuk **melatih Generator**.\n","1.  **Inisialisasi Model GAN**: Sebuah model `Sequential` baru dibuat dengan nama `GAN`.\n","2.  **Membekukan Diskriminator**: `discriminator.trainable = False` adalah langkah **kunci**. Ini memastikan bahwa ketika kita melatih model `GAN`, hanya bobot (parameter) **Generator** yang akan diperbarui. Bobot Diskriminator tetap tidak berubah selama fase pelatihan Generator ini.\n","3.  **Menyusun Model**: `GAN.add(generator)` menambahkan Generator sebagai layer pertama, dan `GAN.add(discriminator)` menambahkan Diskriminator (yang sudah dibekukan) sebagai layer kedua. Alur datanya menjadi: *Input Acak -> Generator -> Gambar Palsu -> Diskriminator -> Output Probabilitas*.\n","4.  **Kompilasi Model GAN**: `GAN.compile(...)` menyiapkan model gabungan ini.\n","    * `loss='binary_crossentropy'`: Sama seperti Diskriminator, karena output akhirnya berasal dari Diskriminator.\n","    * `optimizer=adam`: Menggunakan optimizer yang sama. Ketika `GAN.train_on_batch` dipanggil nanti, optimizer ini akan menghitung gradien berdasarkan *loss*, tetapi *hanya* menerapkan pembaruan pada bobot Generator (karena Diskriminator sudah dibekukan).\n","5.  **Ringkasan GAN**: `GAN.summary()` (di sel berikutnya) akan menunjukkan struktur gabungan ini.\n","\n","**Output/Efek:**\n","* Model Keras `GAN` dibuat, menggabungkan `generator` dan `discriminator`.\n","* Model `GAN` dikompilasi.\n","* Output sel berikutnya (`GAN.summary()`) menunjukkan dua \"layer\" utama (yaitu, model `sequential` (Generator) dan `sequential_1` (Diskriminator)). Yang penting, ia juga menunjukkan jumlah *Trainable params* (parameter yang bisa dilatih) yang sama dengan jumlah parameter Generator, dan *Non-trainable params* yang sama dengan jumlah parameter Diskriminator, memvalidasi bahwa Diskriminator memang telah dibekukan dalam konteks model `GAN` ini."],"metadata":{"id":"CB8PfkNr35oJ"}},{"cell_type":"markdown","metadata":{"id":"2WaNhBDwRwTG"},"source":["## **6) Outputting Images**\n"]},{"cell_type":"code","metadata":{"id":"HQEJ0WbjRppy"},"source":["import matplotlib.pyplot as plt\n","import glob\n","import imageio\n","import PIL\n","\n","save_name = 0.00000000\n","\n","def save_imgs(epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, latent_dim))\n","    gen_imgs = generator.predict(noise)\n","    global save_name\n","    save_name += 0.00000001\n","    # print(\"%.8f\" % save_name)\n","\n","    # Rescale images 0 - 1\n","    # gen_imgs = 0.5 * gen_imgs + 0.5\n","    gen_imgs = (gen_imgs + 1) / 2.0\n","    # gen_imgs = gen_imgs * 255\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt])\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 6: Fungsi Menyimpan Gambar**\n","\n","**Logika Kode:**\n","Blok ini mendefinisikan sebuah fungsi bernama `save_imgs(epoch)` yang akan dipanggil secara berkala selama pelatihan untuk memvisualisasikan hasil Generator.\n","1.  **Parameter**: Fungsi menerima nomor `epoch` saat ini sebagai input (meskipun tidak digunakan dalam nama file di kode ini).\n","2.  **Grid & Noise**: Menentukan ukuran grid untuk gambar (5x5 = 25 gambar). Membuat *noise* acak (`np.random.normal`) sebanyak 25 sampel, masing-masing berukuran `latent_dim` (100).\n","3.  **Generate Images**: Memanggil `generator.predict(noise)` untuk menghasilkan 25 gambar palsu dari *noise* acak tadi.\n","4.  **Filename**: Menggunakan variabel global `save_name` dan menambahkan nilai kecil (0.00000001) setiap kali fungsi dipanggil untuk membuat nama file yang unik secara berurutan (misal: `0.00000001.png`, `0.00000002.png`, dst.).\n","5.  **Rescale Images**: Gambar yang dihasilkan Generator memiliki nilai piksel antara -1 dan 1 (karena aktivasi `tanh`). Baris `gen_imgs = (gen_imgs + 1) / 2.0` mengubah skala nilai ini menjadi antara **0 dan 1**, yang merupakan rentang yang dibutuhkan oleh `matplotlib.pyplot.imshow` untuk menampilkan gambar dengan benar.\n","6.  **Plotting**: Membuat grid plot menggunakan `plt.subplots`. Kemudian, melakukan *loop* untuk menampilkan setiap gambar (`gen_imgs[cnt]`) pada subplot yang sesuai (`axs[i,j]`). Sumbu dinonaktifkan (`axis('off')`).\n","7.  **Saving**: Menyimpan figure plot (`fig`) sebagai file PNG ke dalam folder `generated_images` dengan nama file yang sudah dibuat.\n","8.  **Close Plot**: `plt.close()` menutup figure agar tidak ditampilkan langsung di output notebook dan menghemat memori.\n","\n","**Output/Efek:**\n","* Blok ini hanya **mendefinisikan** fungsi `save_imgs`. Tidak ada output visual saat sel ini dijalankan. Fungsi ini baru akan dieksekusi ketika dipanggil dari dalam *loop* pelatihan di bagian 7. Ketika dipanggil, efeknya adalah pembuatan file gambar PNG di folder `generated_images`."],"metadata":{"id":"xMpbYzgI8Lqx"}},{"cell_type":"markdown","metadata":{"id":"tE57Lk5V0xs2"},"source":["## **7) Training GAN**"]},{"cell_type":"code","source":["def train(epochs, batch_size = 32, save_interval = 500, data_limit=1000):\n","    (X_train, _), (_, _) = cifar10.load_data()\n","\n","    # Rescaling the data\n","    X_train = X_train / 127.5 -1.\n","\n","    # Jika data_limit diberikan, batasi jumlah data yang digunakan\n","    if data_limit is not None:\n","        X_train = X_train[:data_limit]\n","\n","    bat_per_epo = int(X_train.shape[0] / batch_size)\n","\n","    # Create Y label for NN\n","    valid = np.ones((batch_size,1))\n","    fakes = np.zeros((batch_size, 1))\n","\n","    for epoch in range(epochs):\n","        for j in range(bat_per_epo):\n","            # Get Random Batch\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Generate Fake Images\n","            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","            gen_imgs = generator.predict(noise)\n","\n","            # Train Discriminator\n","            d_loss_real = discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","\n","            # Inverse Y label\n","            g_loss = GAN.train_on_batch(noise, valid)\n","\n","            print(\"******* %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss[0]))\n","        save_imgs(epoch)\n","\n","# Batas data menjadi 1000 gambar\n","train(epochs=10, batch_size=32, save_interval=500, data_limit=1000) #original code: train(epochs=1000, batch_size=64, save_interval=200)\n"],"metadata":{"id":"LJc1MkJV7OVr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"88040520-5b5d-43e7-cbbc-922038d390b6","executionInfo":{"status":"ok","timestamp":1729670277622,"user_tz":-420,"elapsed":1287826,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n","  warnings.warn(\"The model does not have any trainable weights.\")\n"]},{"output_type":"stream","name":"stdout","text":["******* 0 [D loss: 0.710809, acc: 29.69%] [G loss: 0.704792]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7a7b50eb0f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7a7b4f3f13f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["******* 0 [D loss: 0.703522, acc: 36.20%] [G loss: 0.702097]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 0 [D loss: 0.704500, acc: 26.93%] [G loss: 0.703581]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 0 [D loss: 0.704324, acc: 21.34%] [G loss: 0.703724]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 0 [D loss: 0.704880, acc: 17.48%] [G loss: 0.704426]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n","******* 0 [D loss: 0.705070, acc: 15.52%] [G loss: 0.704744]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 0 [D loss: 0.705218, acc: 14.14%] [G loss: 0.704997]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 0 [D loss: 0.704971, acc: 13.52%] [G loss: 0.704849]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n","******* 0 [D loss: 0.705620, acc: 12.33%] [G loss: 0.705558]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 0 [D loss: 0.706210, acc: 11.39%] [G loss: 0.706247]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 0 [D loss: 0.706940, acc: 11.05%] [G loss: 0.707049]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 0 [D loss: 0.707540, acc: 10.78%] [G loss: 0.707751]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 0 [D loss: 0.708353, acc: 10.30%] [G loss: 0.708591]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 0 [D loss: 0.709388, acc: 9.78%] [G loss: 0.709767]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 0 [D loss: 0.710439, acc: 9.64%] [G loss: 0.710899]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","******* 0 [D loss: 0.711524, acc: 9.53%] [G loss: 0.712040]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 0 [D loss: 0.712941, acc: 9.05%] [G loss: 0.713580]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n","******* 0 [D loss: 0.714345, acc: 8.63%] [G loss: 0.715046]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 0 [D loss: 0.715950, acc: 8.58%] [G loss: 0.716774]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n","******* 0 [D loss: 0.717592, acc: 8.47%] [G loss: 0.718436]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 0 [D loss: 0.719236, acc: 8.21%] [G loss: 0.720138]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 0 [D loss: 0.720964, acc: 7.98%] [G loss: 0.721975]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n","******* 0 [D loss: 0.722762, acc: 8.04%] [G loss: 0.723812]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 0 [D loss: 0.724709, acc: 7.83%] [G loss: 0.725837]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 0 [D loss: 0.726825, acc: 7.70%] [G loss: 0.727986]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 0 [D loss: 0.728886, acc: 7.65%] [G loss: 0.730052]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n","******* 0 [D loss: 0.730959, acc: 7.59%] [G loss: 0.732128]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 0 [D loss: 0.733206, acc: 7.43%] [G loss: 0.734452]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 0 [D loss: 0.735452, acc: 7.39%] [G loss: 0.736716]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 0 [D loss: 0.737628, acc: 7.25%] [G loss: 0.738866]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 0 [D loss: 0.739756, acc: 7.17%] [G loss: 0.741016]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 1 [D loss: 0.741782, acc: 7.14%] [G loss: 0.743090]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n","******* 1 [D loss: 0.743993, acc: 7.01%] [G loss: 0.745268]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 1 [D loss: 0.745991, acc: 6.94%] [G loss: 0.747236]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 1 [D loss: 0.748043, acc: 6.92%] [G loss: 0.749348]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n","******* 1 [D loss: 0.750206, acc: 6.82%] [G loss: 0.751527]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 1 [D loss: 0.752368, acc: 6.85%] [G loss: 0.753733]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 1 [D loss: 0.754590, acc: 6.79%] [G loss: 0.755944]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n","******* 1 [D loss: 0.756746, acc: 6.73%] [G loss: 0.758092]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 1 [D loss: 0.758884, acc: 6.60%] [G loss: 0.760248]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 1 [D loss: 0.761041, acc: 6.56%] [G loss: 0.762439]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 1 [D loss: 0.763254, acc: 6.55%] [G loss: 0.764642]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n","******* 1 [D loss: 0.765377, acc: 6.51%] [G loss: 0.766728]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 1 [D loss: 0.767476, acc: 6.43%] [G loss: 0.768813]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 1 [D loss: 0.769545, acc: 6.39%] [G loss: 0.770895]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 1 [D loss: 0.771580, acc: 6.39%] [G loss: 0.772897]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 1 [D loss: 0.773611, acc: 6.38%] [G loss: 0.774917]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 1 [D loss: 0.775555, acc: 6.48%] [G loss: 0.776910]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 1 [D loss: 0.777599, acc: 6.44%] [G loss: 0.778903]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n","******* 1 [D loss: 0.779531, acc: 6.44%] [G loss: 0.780838]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 1 [D loss: 0.781507, acc: 6.40%] [G loss: 0.782845]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 1 [D loss: 0.783586, acc: 6.37%] [G loss: 0.784896]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 1 [D loss: 0.785621, acc: 6.31%] [G loss: 0.786958]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 1 [D loss: 0.787494, acc: 6.40%] [G loss: 0.788715]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 1 [D loss: 0.789286, acc: 6.39%] [G loss: 0.790556]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 1 [D loss: 0.791052, acc: 6.42%] [G loss: 0.792277]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n","******* 1 [D loss: 0.792838, acc: 6.39%] [G loss: 0.794106]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 1 [D loss: 0.794641, acc: 6.41%] [G loss: 0.795927]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 1 [D loss: 0.796499, acc: 6.36%] [G loss: 0.797750]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n","******* 1 [D loss: 0.798314, acc: 6.25%] [G loss: 0.799563]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 1 [D loss: 0.800204, acc: 6.15%] [G loss: 0.801456]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 1 [D loss: 0.801967, acc: 6.07%] [G loss: 0.803201]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n","******* 2 [D loss: 0.803775, acc: 6.03%] [G loss: 0.805034]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.805480, acc: 6.05%] [G loss: 0.806646]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 2 [D loss: 0.807170, acc: 6.11%] [G loss: 0.808400]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 2 [D loss: 0.808985, acc: 6.04%] [G loss: 0.810235]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 2 [D loss: 0.810758, acc: 6.13%] [G loss: 0.811995]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.812468, acc: 6.11%] [G loss: 0.813690]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 2 [D loss: 0.814117, acc: 6.11%] [G loss: 0.815315]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n","******* 2 [D loss: 0.815785, acc: 6.07%] [G loss: 0.817008]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 2 [D loss: 0.817521, acc: 6.07%] [G loss: 0.818756]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.819232, acc: 6.08%] [G loss: 0.820432]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.820873, acc: 6.04%] [G loss: 0.822040]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 2 [D loss: 0.822527, acc: 6.04%] [G loss: 0.823695]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 2 [D loss: 0.824174, acc: 6.00%] [G loss: 0.825390]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n","******* 2 [D loss: 0.825817, acc: 5.98%] [G loss: 0.826965]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step\n","******* 2 [D loss: 0.827382, acc: 5.97%] [G loss: 0.828544]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 2 [D loss: 0.829001, acc: 5.93%] [G loss: 0.830163]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 2 [D loss: 0.830609, acc: 5.93%] [G loss: 0.831777]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n","******* 2 [D loss: 0.832162, acc: 5.94%] [G loss: 0.833305]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.833711, acc: 5.94%] [G loss: 0.834859]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 2 [D loss: 0.835281, acc: 5.93%] [G loss: 0.836405]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 2 [D loss: 0.836818, acc: 5.91%] [G loss: 0.837960]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n","******* 2 [D loss: 0.838339, acc: 5.88%] [G loss: 0.839436]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.839823, acc: 5.83%] [G loss: 0.840921]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 2 [D loss: 0.841256, acc: 5.78%] [G loss: 0.842321]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n","******* 2 [D loss: 0.842643, acc: 5.80%] [G loss: 0.843698]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 2 [D loss: 0.844041, acc: 5.77%] [G loss: 0.845104]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","******* 2 [D loss: 0.845479, acc: 5.74%] [G loss: 0.846576]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 2 [D loss: 0.846834, acc: 5.75%] [G loss: 0.847861]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 2 [D loss: 0.848205, acc: 5.70%] [G loss: 0.849241]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 2 [D loss: 0.849555, acc: 5.67%] [G loss: 0.850619]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 2 [D loss: 0.850926, acc: 5.64%] [G loss: 0.851954]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n","******* 3 [D loss: 0.852225, acc: 5.63%] [G loss: 0.853235]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 3 [D loss: 0.853507, acc: 5.62%] [G loss: 0.854506]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 3 [D loss: 0.854794, acc: 5.61%] [G loss: 0.855820]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n","******* 3 [D loss: 0.856066, acc: 5.65%] [G loss: 0.857038]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 3 [D loss: 0.857314, acc: 5.67%] [G loss: 0.858311]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n","******* 3 [D loss: 0.858575, acc: 5.65%] [G loss: 0.859549]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 3 [D loss: 0.859825, acc: 5.62%] [G loss: 0.860791]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 3 [D loss: 0.861085, acc: 5.58%] [G loss: 0.862080]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 3 [D loss: 0.862332, acc: 5.56%] [G loss: 0.863311]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 3 [D loss: 0.863563, acc: 5.58%] [G loss: 0.864527]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n","******* 3 [D loss: 0.864728, acc: 5.57%] [G loss: 0.865652]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 3 [D loss: 0.865900, acc: 5.56%] [G loss: 0.866852]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 3 [D loss: 0.867011, acc: 5.63%] [G loss: 0.867925]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n","******* 3 [D loss: 0.868136, acc: 5.61%] [G loss: 0.869065]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 3 [D loss: 0.869300, acc: 5.58%] [G loss: 0.870239]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 3 [D loss: 0.870452, acc: 5.56%] [G loss: 0.871373]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","******* 3 [D loss: 0.871569, acc: 5.58%] [G loss: 0.872497]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 3 [D loss: 0.872692, acc: 5.54%] [G loss: 0.873586]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 3 [D loss: 0.873769, acc: 5.54%] [G loss: 0.874669]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 3 [D loss: 0.874866, acc: 5.52%] [G loss: 0.875765]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n","******* 3 [D loss: 0.875978, acc: 5.49%] [G loss: 0.876872]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 3 [D loss: 0.877094, acc: 5.46%] [G loss: 0.878014]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 3 [D loss: 0.878178, acc: 5.47%] [G loss: 0.879057]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 3 [D loss: 0.879257, acc: 5.46%] [G loss: 0.880139]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n","******* 3 [D loss: 0.880337, acc: 5.41%] [G loss: 0.881216]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 3 [D loss: 0.881378, acc: 5.41%] [G loss: 0.882244]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n","******* 3 [D loss: 0.882401, acc: 5.40%] [G loss: 0.883246]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 3 [D loss: 0.883419, acc: 5.40%] [G loss: 0.884271]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 3 [D loss: 0.884438, acc: 5.39%] [G loss: 0.885300]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 3 [D loss: 0.885422, acc: 5.40%] [G loss: 0.886271]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 3 [D loss: 0.886401, acc: 5.39%] [G loss: 0.887231]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n","******* 4 [D loss: 0.887381, acc: 5.37%] [G loss: 0.888214]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 4 [D loss: 0.888358, acc: 5.36%] [G loss: 0.889183]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 4 [D loss: 0.889361, acc: 5.39%] [G loss: 0.890211]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 4 [D loss: 0.890375, acc: 5.38%] [G loss: 0.891224]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 4 [D loss: 0.891377, acc: 5.35%] [G loss: 0.892196]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n","******* 4 [D loss: 0.892321, acc: 5.32%] [G loss: 0.893133]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n","******* 4 [D loss: 0.893259, acc: 5.33%] [G loss: 0.894079]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 4 [D loss: 0.894230, acc: 5.29%] [G loss: 0.895044]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n","******* 4 [D loss: 0.895186, acc: 5.27%] [G loss: 0.895987]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 4 [D loss: 0.896145, acc: 5.27%] [G loss: 0.896954]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 4 [D loss: 0.897076, acc: 5.26%] [G loss: 0.897878]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 4 [D loss: 0.897956, acc: 5.26%] [G loss: 0.898729]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step\n","******* 4 [D loss: 0.898853, acc: 5.22%] [G loss: 0.899626]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 4 [D loss: 0.899758, acc: 5.22%] [G loss: 0.900555]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 4 [D loss: 0.900638, acc: 5.23%] [G loss: 0.901405]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n","******* 4 [D loss: 0.901517, acc: 5.20%] [G loss: 0.902293]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 4 [D loss: 0.902369, acc: 5.20%] [G loss: 0.903121]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 4 [D loss: 0.903227, acc: 5.21%] [G loss: 0.903993]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n","******* 4 [D loss: 0.904109, acc: 5.20%] [G loss: 0.904886]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 4 [D loss: 0.904976, acc: 5.17%] [G loss: 0.905729]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 4 [D loss: 0.905820, acc: 5.15%] [G loss: 0.906556]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 4 [D loss: 0.906651, acc: 5.15%] [G loss: 0.907413]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 4 [D loss: 0.907477, acc: 5.13%] [G loss: 0.908200]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 4 [D loss: 0.908272, acc: 5.14%] [G loss: 0.909008]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 4 [D loss: 0.909067, acc: 5.14%] [G loss: 0.909793]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n","******* 4 [D loss: 0.909856, acc: 5.13%] [G loss: 0.910577]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","******* 4 [D loss: 0.910653, acc: 5.12%] [G loss: 0.911366]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 4 [D loss: 0.911385, acc: 5.15%] [G loss: 0.912079]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n","******* 4 [D loss: 0.912138, acc: 5.17%] [G loss: 0.912856]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 4 [D loss: 0.912901, acc: 5.15%] [G loss: 0.913607]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 4 [D loss: 0.913656, acc: 5.14%] [G loss: 0.914339]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n","******* 5 [D loss: 0.914410, acc: 5.13%] [G loss: 0.915117]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 5 [D loss: 0.915167, acc: 5.11%] [G loss: 0.915851]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 5 [D loss: 0.915931, acc: 5.10%] [G loss: 0.916638]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 5 [D loss: 0.916686, acc: 5.12%] [G loss: 0.917377]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 5 [D loss: 0.917418, acc: 5.12%] [G loss: 0.918103]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 5 [D loss: 0.918169, acc: 5.11%] [G loss: 0.918860]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 5 [D loss: 0.918897, acc: 5.15%] [G loss: 0.919569]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n","******* 5 [D loss: 0.919608, acc: 5.17%] [G loss: 0.920288]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 5 [D loss: 0.920321, acc: 5.17%] [G loss: 0.920982]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n","******* 5 [D loss: 0.921024, acc: 5.19%] [G loss: 0.921693]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n","******* 5 [D loss: 0.921735, acc: 5.18%] [G loss: 0.922399]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 5 [D loss: 0.922458, acc: 5.19%] [G loss: 0.923130]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 5 [D loss: 0.923164, acc: 5.20%] [G loss: 0.923814]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 5 [D loss: 0.923846, acc: 5.23%] [G loss: 0.924514]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 5 [D loss: 0.924563, acc: 5.25%] [G loss: 0.925224]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 5 [D loss: 0.925261, acc: 5.24%] [G loss: 0.925905]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 5 [D loss: 0.925927, acc: 5.24%] [G loss: 0.926577]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n","******* 5 [D loss: 0.926601, acc: 5.24%] [G loss: 0.927246]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 5 [D loss: 0.927277, acc: 5.22%] [G loss: 0.927909]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 5 [D loss: 0.927940, acc: 5.24%] [G loss: 0.928597]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n","******* 5 [D loss: 0.928623, acc: 5.25%] [G loss: 0.929245]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 5 [D loss: 0.929263, acc: 5.24%] [G loss: 0.929893]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 5 [D loss: 0.929909, acc: 5.25%] [G loss: 0.930528]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 5 [D loss: 0.930562, acc: 5.27%] [G loss: 0.931194]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 5 [D loss: 0.931221, acc: 5.28%] [G loss: 0.931841]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 5 [D loss: 0.931831, acc: 5.30%] [G loss: 0.932435]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 5 [D loss: 0.932454, acc: 5.30%] [G loss: 0.933075]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n","******* 5 [D loss: 0.933097, acc: 5.33%] [G loss: 0.933711]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 5 [D loss: 0.933717, acc: 5.32%] [G loss: 0.934326]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 5 [D loss: 0.934314, acc: 5.34%] [G loss: 0.934912]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n","******* 5 [D loss: 0.934914, acc: 5.32%] [G loss: 0.935501]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 6 [D loss: 0.935510, acc: 5.35%] [G loss: 0.936116]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 6 [D loss: 0.936131, acc: 5.35%] [G loss: 0.936728]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n","******* 6 [D loss: 0.936734, acc: 5.34%] [G loss: 0.937335]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 6 [D loss: 0.937359, acc: 5.32%] [G loss: 0.937956]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 6 [D loss: 0.937937, acc: 5.34%] [G loss: 0.938518]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 6 [D loss: 0.938502, acc: 5.35%] [G loss: 0.939080]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n","******* 6 [D loss: 0.939081, acc: 5.34%] [G loss: 0.939658]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 6 [D loss: 0.939667, acc: 5.33%] [G loss: 0.940257]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 6 [D loss: 0.940236, acc: 5.33%] [G loss: 0.940808]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n","******* 6 [D loss: 0.940839, acc: 5.32%] [G loss: 0.941444]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 6 [D loss: 0.941440, acc: 5.30%] [G loss: 0.942012]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 6 [D loss: 0.942026, acc: 5.32%] [G loss: 0.942619]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n","******* 6 [D loss: 0.942603, acc: 5.32%] [G loss: 0.943160]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 6 [D loss: 0.943177, acc: 5.31%] [G loss: 0.943760]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 6 [D loss: 0.943745, acc: 5.31%] [G loss: 0.944301]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 6 [D loss: 0.944298, acc: 5.32%] [G loss: 0.944869]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 6 [D loss: 0.944847, acc: 5.31%] [G loss: 0.945400]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 6 [D loss: 0.945360, acc: 5.31%] [G loss: 0.945899]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 6 [D loss: 0.945897, acc: 5.30%] [G loss: 0.946456]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n","******* 6 [D loss: 0.946447, acc: 5.29%] [G loss: 0.947003]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n","******* 6 [D loss: 0.946979, acc: 5.28%] [G loss: 0.947526]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 6 [D loss: 0.947494, acc: 5.26%] [G loss: 0.948022]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 6 [D loss: 0.947993, acc: 5.27%] [G loss: 0.948522]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 6 [D loss: 0.948498, acc: 5.27%] [G loss: 0.949036]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 6 [D loss: 0.949013, acc: 5.27%] [G loss: 0.949555]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 6 [D loss: 0.949526, acc: 5.28%] [G loss: 0.950055]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 6 [D loss: 0.950035, acc: 5.28%] [G loss: 0.950578]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 6 [D loss: 0.950547, acc: 5.29%] [G loss: 0.951077]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 6 [D loss: 0.951065, acc: 5.28%] [G loss: 0.951607]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n","******* 6 [D loss: 0.951605, acc: 5.27%] [G loss: 0.952147]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 6 [D loss: 0.952133, acc: 5.26%] [G loss: 0.952662]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 7 [D loss: 0.952651, acc: 5.27%] [G loss: 0.953188]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 7 [D loss: 0.953164, acc: 5.26%] [G loss: 0.953686]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 7 [D loss: 0.953663, acc: 5.25%] [G loss: 0.954180]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 7 [D loss: 0.954172, acc: 5.25%] [G loss: 0.954700]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 7 [D loss: 0.954678, acc: 5.25%] [G loss: 0.955193]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n","******* 7 [D loss: 0.955192, acc: 5.25%] [G loss: 0.955723]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 7 [D loss: 0.955699, acc: 5.22%] [G loss: 0.956215]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 7 [D loss: 0.956192, acc: 5.23%] [G loss: 0.956714]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","******* 7 [D loss: 0.956690, acc: 5.23%] [G loss: 0.957206]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 7 [D loss: 0.957175, acc: 5.22%] [G loss: 0.957679]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 7 [D loss: 0.957656, acc: 5.21%] [G loss: 0.958155]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 7 [D loss: 0.958132, acc: 5.20%] [G loss: 0.958628]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n","******* 7 [D loss: 0.958622, acc: 5.19%] [G loss: 0.959141]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n","******* 7 [D loss: 0.959108, acc: 5.18%] [G loss: 0.959603]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n","******* 7 [D loss: 0.959567, acc: 5.20%] [G loss: 0.960070]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n","******* 7 [D loss: 0.960039, acc: 5.22%] [G loss: 0.960535]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 7 [D loss: 0.960514, acc: 5.21%] [G loss: 0.961016]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","******* 7 [D loss: 0.960978, acc: 5.22%] [G loss: 0.961471]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 7 [D loss: 0.961421, acc: 5.22%] [G loss: 0.961894]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 7 [D loss: 0.961862, acc: 5.22%] [G loss: 0.962351]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 7 [D loss: 0.962316, acc: 5.21%] [G loss: 0.962803]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n","******* 7 [D loss: 0.962776, acc: 5.22%] [G loss: 0.963273]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 7 [D loss: 0.963246, acc: 5.21%] [G loss: 0.963731]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 7 [D loss: 0.963698, acc: 5.21%] [G loss: 0.964183]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n","******* 7 [D loss: 0.964150, acc: 5.20%] [G loss: 0.964629]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 7 [D loss: 0.964613, acc: 5.18%] [G loss: 0.965102]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 7 [D loss: 0.965051, acc: 5.17%] [G loss: 0.965515]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n","******* 7 [D loss: 0.965483, acc: 5.16%] [G loss: 0.965959]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n","******* 7 [D loss: 0.965899, acc: 5.18%] [G loss: 0.966361]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 7 [D loss: 0.966331, acc: 5.19%] [G loss: 0.966804]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 7 [D loss: 0.966787, acc: 5.18%] [G loss: 0.967273]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n","******* 8 [D loss: 0.967240, acc: 5.18%] [G loss: 0.967706]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n","******* 8 [D loss: 0.967668, acc: 5.17%] [G loss: 0.968128]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 8 [D loss: 0.968079, acc: 5.16%] [G loss: 0.968531]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n","******* 8 [D loss: 0.968495, acc: 5.14%] [G loss: 0.968955]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n","******* 8 [D loss: 0.968927, acc: 5.13%] [G loss: 0.969400]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n","******* 8 [D loss: 0.969359, acc: 5.13%] [G loss: 0.969820]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 8 [D loss: 0.969798, acc: 5.14%] [G loss: 0.970270]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n","******* 8 [D loss: 0.970245, acc: 5.15%] [G loss: 0.970721]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 8 [D loss: 0.970693, acc: 5.15%] [G loss: 0.971155]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 8 [D loss: 0.971115, acc: 5.15%] [G loss: 0.971578]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 8 [D loss: 0.971538, acc: 5.15%] [G loss: 0.971994]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n","******* 8 [D loss: 0.971946, acc: 5.14%] [G loss: 0.972396]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 8 [D loss: 0.972342, acc: 5.16%] [G loss: 0.972792]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n","******* 8 [D loss: 0.972751, acc: 5.17%] [G loss: 0.973209]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 8 [D loss: 0.973160, acc: 5.17%] [G loss: 0.973603]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 8 [D loss: 0.973553, acc: 5.17%] [G loss: 0.973997]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 8 [D loss: 0.973956, acc: 5.16%] [G loss: 0.974393]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 8 [D loss: 0.974340, acc: 5.15%] [G loss: 0.974771]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 8 [D loss: 0.974720, acc: 5.16%] [G loss: 0.975160]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 8 [D loss: 0.975113, acc: 5.15%] [G loss: 0.975555]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n","******* 8 [D loss: 0.975512, acc: 5.15%] [G loss: 0.975952]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 8 [D loss: 0.975896, acc: 5.14%] [G loss: 0.976320]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n","******* 8 [D loss: 0.976292, acc: 5.14%] [G loss: 0.976732]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step\n","******* 8 [D loss: 0.976678, acc: 5.14%] [G loss: 0.977116]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 8 [D loss: 0.977069, acc: 5.16%] [G loss: 0.977501]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 8 [D loss: 0.977467, acc: 5.14%] [G loss: 0.977905]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n","******* 8 [D loss: 0.977850, acc: 5.14%] [G loss: 0.978274]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 8 [D loss: 0.978235, acc: 5.12%] [G loss: 0.978664]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 8 [D loss: 0.978605, acc: 5.11%] [G loss: 0.979009]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 8 [D loss: 0.978958, acc: 5.11%] [G loss: 0.979381]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 8 [D loss: 0.979335, acc: 5.10%] [G loss: 0.979758]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n","******* 9 [D loss: 0.979710, acc: 5.11%] [G loss: 0.980136]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n","******* 9 [D loss: 0.980096, acc: 5.10%] [G loss: 0.980523]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 9 [D loss: 0.980479, acc: 5.10%] [G loss: 0.980909]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 9 [D loss: 0.980860, acc: 5.10%] [G loss: 0.981276]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 9 [D loss: 0.981208, acc: 5.10%] [G loss: 0.981619]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n","******* 9 [D loss: 0.981556, acc: 5.10%] [G loss: 0.981967]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 9 [D loss: 0.981907, acc: 5.10%] [G loss: 0.982309]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n","******* 9 [D loss: 0.982249, acc: 5.09%] [G loss: 0.982642]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n","******* 9 [D loss: 0.982588, acc: 5.09%] [G loss: 0.982996]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n","******* 9 [D loss: 0.982952, acc: 5.09%] [G loss: 0.983368]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.983311, acc: 5.10%] [G loss: 0.983716]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n","******* 9 [D loss: 0.983675, acc: 5.09%] [G loss: 0.984091]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 9 [D loss: 0.984038, acc: 5.08%] [G loss: 0.984440]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 9 [D loss: 0.984374, acc: 5.08%] [G loss: 0.984774]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n","******* 9 [D loss: 0.984735, acc: 5.09%] [G loss: 0.985148]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.985088, acc: 5.08%] [G loss: 0.985487]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 9 [D loss: 0.985427, acc: 5.09%] [G loss: 0.985829]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n","******* 9 [D loss: 0.985774, acc: 5.09%] [G loss: 0.986177]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step\n","******* 9 [D loss: 0.986109, acc: 5.09%] [G loss: 0.986501]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n","******* 9 [D loss: 0.986452, acc: 5.08%] [G loss: 0.986854]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n","******* 9 [D loss: 0.986801, acc: 5.07%] [G loss: 0.987200]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n","******* 9 [D loss: 0.987142, acc: 5.07%] [G loss: 0.987537]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n","******* 9 [D loss: 0.987483, acc: 5.06%] [G loss: 0.987876]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.987804, acc: 5.07%] [G loss: 0.988191]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n","******* 9 [D loss: 0.988136, acc: 5.07%] [G loss: 0.988532]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n","******* 9 [D loss: 0.988471, acc: 5.07%] [G loss: 0.988862]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.988805, acc: 5.06%] [G loss: 0.989189]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.989144, acc: 5.07%] [G loss: 0.989541]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n","******* 9 [D loss: 0.989479, acc: 5.06%] [G loss: 0.989860]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n","******* 9 [D loss: 0.989809, acc: 5.06%] [G loss: 0.990203]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n","******* 9 [D loss: 0.990154, acc: 5.05%] [G loss: 0.990537]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n"]}]},{"cell_type":"code","metadata":{"id":"GS9wDLeRLUOB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729670277623,"user_tz":-420,"elapsed":20,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}},"outputId":"1b925b11-8563-417d-e231-0d3c055bd27f"},"source":["noise = np.random.normal(0, 1, (1,latent_dim))\n","gen_imgs = generator.predict(noise)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"]}]},{"cell_type":"code","metadata":{"id":"9Rlgu8g9Lik9","colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"status":"ok","timestamp":1729670278241,"user_tz":-420,"elapsed":631,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}},"outputId":"da19bece-1762-4261-8f77-1a137dc7e0b2"},"source":["gen_imgs = (gen_imgs + 1) / 2.0\n","plt.imshow(gen_imgs[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7a7b4f4eac50>"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 7: Pelatihan GAN**\n","\n","**Logika Kode:**\n","Ini adalah inti dari program, di mana proses pelatihan GAN (Generator vs Diskriminator) terjadi. Fungsi `train` didefinisikan dan kemudian dipanggil.\n","1.  **Definisi `train`**:\n","    * **Parameter**: Menerima jumlah `epochs` (berapa kali iterasi seluruh dataset), `batch_size` (jumlah gambar per langkah update), `save_interval` (frekuensi menyimpan gambar, meskipun tidak digunakan di pemanggilan akhir), dan `data_limit` (opsional, untuk membatasi data training).\n","    * **Load Data**: Memuat dataset CIFAR-10 menggunakan `cifar10.load_data()`. Hanya data gambar (`X_train`) yang diambil, labelnya diabaikan (`_`).\n","    * **Rescale Data**: Gambar asli (nilai piksel 0-255) diubah skalanya menjadi **[-1, 1]** agar cocok dengan rentang output Generator (`tanh`).\n","    * **Limit Data**: Jika `data_limit` diberikan (dalam pemanggilan ini 1000), dataset `X_train` dipotong hanya mengambil 1000 gambar pertama. Ini **mempercepat pelatihan** secara signifikan, cocok untuk demonstrasi atau pengujian cepat, tetapi hasilnya mungkin tidak sebagus melatih dengan data penuh.\n","    * **Batches per Epoch**: Menghitung berapa banyak batch yang ada dalam satu epoch berdasarkan data yang (mungkin) sudah dibatasi.\n","    * **Labels**: Membuat array target: `valid` (berisi angka 1) dan `fakes` (berisi angka 0) sesuai ukuran `batch_size`.\n","    * **Training Loop**:\n","        * **Outer Loop (`epoch`)**: Berjalan sebanyak `epochs` kali.\n","        * **Inner Loop (`j` / batch)**: Berjalan sebanyak `bat_per_epo` kali.\n","            * **Langkah 1: Latih Diskriminator**\n","                * Ambil satu *batch* gambar **asli** (`imgs`) secara acak dari `X_train`.\n","                * Buat satu *batch* gambar **palsu** (`gen_imgs`) menggunakan Generator dari *noise* acak.\n","                * Latih Diskriminator dua kali dalam satu langkah:\n","                    * `discriminator.train_on_batch(imgs, valid)`: Mengajari Diskriminator mengenali gambar asli sebagai \"valid\" (target 1).\n","                    * `discriminator.train_on_batch(gen_imgs, fakes)`: Mengajari Diskriminator mengenali gambar palsu sebagai \"fake\" (target 0).\n","                * Hitung rata-rata *loss* dan *accuracy* Diskriminator (`d_loss`).\n","            * **Langkah 2: Latih Generator**\n","                * Buat *batch* **noise** acak baru.\n","                * Latih model **`GAN`** gabungan (`GAN.train_on_batch(noise, valid)`). Di sini kita memberikan *noise* sebagai input dan **label `valid` (1)** sebagai target. Tujuannya adalah untuk \"menipu\" Diskriminator agar mengira gambar palsu yang dihasilkan dari *noise* itu adalah asli. Karena Diskriminator dibekukan di dalam model `GAN`, *loss* yang dihitung hanya digunakan untuk memperbarui **bobot Generator**.\n","                * Simpan *loss* Generator (`g_loss`).\n","            * **Cetak Progres**: Menampilkan *loss* dan *accuracy* Diskriminator serta *loss* Generator untuk memantau kemajuan pelatihan.\n","        * **Simpan Gambar**: Panggil `save_imgs(epoch)` **setelah setiap epoch selesai** (berdasarkan kode pemanggilan `train` yang dimodifikasi).\n","\n","2.  **Pemanggilan `train`**: `train(epochs=10, batch_size=32, data_limit=1000)` memulai proses pelatihan dengan parameter yang ditentukan: 10 epoch, batch size 32, menggunakan 1000 gambar pertama dari CIFAR-10.\n","\n","**Output/Efek:**\n","* Dataset CIFAR-10 diunduh jika belum ada (output menunjukkan proses unduh).\n","* Loop pelatihan berjalan. Untuk setiap *batch* di setiap *epoch*, baris `******* epoch [D loss: ..., acc: ...] [G loss: ...]` dicetak, menunjukkan nilai *loss* dan akurasi saat itu.\n","* Peringatan (*Warning*) tentang `tf.function retracing` mungkin muncul, ini terkait optimasi internal TensorFlow dan seringkali bisa diabaikan dalam konteks ini.\n","* Peringatan tentang *model does not have trainable weights* saat memprediksi mungkin muncul jika generator belum dilatih sama sekali di awal, ini normal.\n","* Di akhir setiap *epoch* (total 10 kali), fungsi `save_imgs` dipanggil. Ini akan menghasilkan output `predict` dari Generator dan membuat file `0.0000000X.png` di folder `generated_images`.\n","* Setelah 10 epoch, pelatihan selesai. Bobot Generator dan Diskriminator telah diperbarui berdasarkan proses pelatihan. Karena jumlah epoch dan data sangat terbatas, gambar yang dihasilkan kemungkinan besar masih sangat buram dan belum menyerupai objek CIFAR-10 yang jelas."],"metadata":{"id":"HX4uWkYd8Sgo"}},{"cell_type":"markdown","metadata":{"id":"po-jSQoN1Azl"},"source":["### **8) Making GIF**"]},{"cell_type":"code","metadata":{"id":"XPShgQpg1EMy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729670278845,"user_tz":-420,"elapsed":618,"user":{"displayName":"Chandra Wirabuana","userId":"07689470724962073555"}},"outputId":"c6e73cf5-289d-403c-ac59-6d681b5de7a6"},"source":["# Display a single image using the epoch number\n","# def display_image(epoch_no):\n","#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n","\n","anim_file = 'dcgan.gif'\n","\n","with imageio.get_writer(anim_file, mode='I') as writer:\n","  filenames = glob.glob('generated_images/*.png')\n","  filenames = sorted(filenames)\n","  for filename in filenames:\n","    image = imageio.imread(filename)\n","    writer.append_data(image)\n","\n","  if filenames:  # Check if filenames is not empty\n","    image = imageio.imread(filenames[-1])\n","    writer.append_data(image)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-46aed9259859>:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  image = imageio.imread(filename)\n","<ipython-input-11-46aed9259859>:15: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  image = imageio.imread(filenames[-1])\n"]}]},{"cell_type":"markdown","source":["### **Penjelasan Logika & Output Bagian 8: Membuat GIF**\n","\n","**Logika Kode:**\n","Blok terakhir ini bertujuan untuk membuat file **GIF animasi** dari gambar-gambar PNG yang telah disimpan di folder `generated_images` selama pelatihan.\n","1.  **Nama File GIF**: Menentukan nama file output sebagai `dcgan.gif`.\n","2.  **Cari File Gambar**: `glob.glob('generated_images/*.png')` mencari semua file yang berakhiran `.png` di dalam folder `generated_images`. Hasilnya adalah daftar nama file.\n","3.  **Urutkan File**: `filenames = sorted(filenames)` mengurutkan nama file. Karena nama filenya adalah angka desimal yang bertambah (misal `0.00000001.png`, `0.00000002.png`), pengurutan *string* standar akan menghasilkan urutan yang benar secara kronologis.\n","4.  **Buka Penulis GIF**: `imageio.get_writer(anim_file, mode='I')` membuka file GIF untuk ditulis. `mode='I'` biasanya berarti *multiple images*.\n","5.  **Loop & Tambahkan Gambar**: Kode melakukan *loop* melalui setiap `filename` yang sudah diurutkan:\n","    * `image = imageio.imread(filename)`: Membaca file PNG menjadi data gambar (array NumPy).\n","    * `writer.append_data(image)`: Menambahkan *frame* gambar tersebut ke file GIF.\n","6.  **Tambahkan Frame Terakhir (Opsional)**: Setelah *loop*, kode memeriksa apakah ada file yang ditemukan (`if filenames:`). Jika ya, ia membaca **gambar terakhir** sekali lagi dan menambahkannya ke GIF. Ini sering dilakukan agar animasi berhenti sejenak di *frame* terakhir sebelum mengulang.\n","\n","**Output/Efek:**\n","* Sebuah file GIF animasi bernama `dcgan.gif` dibuat di direktori kerja saat ini.\n","* GIF ini akan menampilkan urutan gambar sampel yang dihasilkan Generator di setiap *epoch* (atau interval penyimpanan), menunjukkan evolusi kemampuan Generator selama pelatihan.\n","* Peringatan (*DeprecationWarning*) terkait `imageio.imread` mungkin muncul, menyarankan penggunaan versi baru `imageio.v3.imread`, tetapi kode biasanya tetap berfungsi. Jika tidak ada file PNG yang ditemukan di `generated_images`, GIF tidak akan dibuat atau akan kosong."],"metadata":{"id":"oyJAP2HF8V3q"}}]}